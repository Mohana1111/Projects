{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ff1ed0-839a-4eda-b63c-2d3030fb0327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygame in /Users/mohana25/Library/Python/3.9/lib/python/site-packages (2.5.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194ffe1-eafd-4dd7-8aa1-9995532f3dee",
   "metadata": {},
   "source": [
    "#### ***INVERTED PENDULUM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9787082-163a-48e6-9d4d-4caab4041a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import pygame\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec559aeb-530e-4cac-9bb6-9a16a6ad1270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 42.0\n",
      "Episode 1, Reward: 14.0\n",
      "Episode 2, Reward: 54.0\n",
      "Episode 3, Reward: 11.0\n",
      "Episode 4, Reward: 11.0\n",
      "Episode 5, Reward: 35.0\n",
      "Episode 6, Reward: 17.0\n",
      "Episode 7, Reward: 11.0\n",
      "Episode 8, Reward: 29.0\n",
      "Episode 9, Reward: 9.0\n",
      "Episode 10, Reward: 24.0\n",
      "Episode 11, Reward: 59.0\n",
      "Episode 12, Reward: 20.0\n",
      "Episode 13, Reward: 17.0\n",
      "Episode 14, Reward: 20.0\n",
      "Episode 15, Reward: 12.0\n",
      "Episode 16, Reward: 15.0\n",
      "Episode 17, Reward: 14.0\n",
      "Episode 18, Reward: 13.0\n",
      "Episode 19, Reward: 18.0\n",
      "Episode 20, Reward: 18.0\n",
      "Episode 21, Reward: 23.0\n",
      "Episode 22, Reward: 25.0\n",
      "Episode 23, Reward: 26.0\n",
      "Episode 24, Reward: 15.0\n",
      "Episode 25, Reward: 13.0\n",
      "Episode 26, Reward: 43.0\n",
      "Episode 27, Reward: 34.0\n",
      "Episode 28, Reward: 16.0\n",
      "Episode 29, Reward: 16.0\n",
      "Episode 30, Reward: 9.0\n",
      "Episode 31, Reward: 12.0\n",
      "Episode 32, Reward: 62.0\n",
      "Episode 33, Reward: 17.0\n",
      "Episode 34, Reward: 51.0\n",
      "Episode 35, Reward: 49.0\n",
      "Episode 36, Reward: 27.0\n",
      "Episode 37, Reward: 17.0\n",
      "Episode 38, Reward: 48.0\n",
      "Episode 39, Reward: 9.0\n",
      "Episode 40, Reward: 46.0\n",
      "Episode 41, Reward: 22.0\n",
      "Episode 42, Reward: 18.0\n",
      "Episode 43, Reward: 32.0\n",
      "Episode 44, Reward: 71.0\n",
      "Episode 45, Reward: 33.0\n",
      "Episode 46, Reward: 37.0\n",
      "Episode 47, Reward: 9.0\n",
      "Episode 48, Reward: 80.0\n",
      "Episode 49, Reward: 15.0\n",
      "Episode 50, Reward: 19.0\n",
      "Episode 51, Reward: 78.0\n",
      "Episode 52, Reward: 13.0\n",
      "Episode 53, Reward: 74.0\n",
      "Episode 54, Reward: 100.0\n",
      "Episode 55, Reward: 95.0\n",
      "Episode 56, Reward: 14.0\n",
      "Episode 57, Reward: 15.0\n",
      "Episode 58, Reward: 52.0\n",
      "Episode 59, Reward: 147.0\n",
      "Episode 60, Reward: 74.0\n",
      "Episode 61, Reward: 147.0\n",
      "Episode 62, Reward: 35.0\n",
      "Episode 63, Reward: 136.0\n",
      "Episode 64, Reward: 120.0\n",
      "Episode 65, Reward: 130.0\n",
      "Episode 66, Reward: 121.0\n",
      "Episode 67, Reward: 29.0\n",
      "Episode 68, Reward: 59.0\n",
      "Episode 69, Reward: 40.0\n",
      "Episode 70, Reward: 42.0\n",
      "Episode 71, Reward: 224.0\n",
      "Episode 72, Reward: 83.0\n",
      "Episode 73, Reward: 153.0\n",
      "Episode 74, Reward: 105.0\n",
      "Episode 75, Reward: 27.0\n",
      "Episode 76, Reward: 149.0\n",
      "Episode 77, Reward: 73.0\n",
      "Episode 78, Reward: 173.0\n",
      "Episode 79, Reward: 113.0\n",
      "Episode 80, Reward: 153.0\n",
      "Episode 81, Reward: 178.0\n",
      "Episode 82, Reward: 214.0\n",
      "Episode 83, Reward: 355.0\n",
      "Episode 84, Reward: 149.0\n",
      "Episode 85, Reward: 146.0\n",
      "Episode 86, Reward: 227.0\n",
      "Episode 87, Reward: 42.0\n",
      "Episode 88, Reward: 28.0\n",
      "Episode 89, Reward: 76.0\n",
      "Episode 90, Reward: 170.0\n",
      "Episode 91, Reward: 66.0\n",
      "Episode 92, Reward: 135.0\n",
      "Episode 93, Reward: 31.0\n",
      "Episode 94, Reward: 80.0\n",
      "Episode 95, Reward: 124.0\n",
      "Episode 96, Reward: 22.0\n",
      "Episode 97, Reward: 135.0\n",
      "Episode 98, Reward: 51.0\n",
      "Episode 99, Reward: 119.0\n",
      "Episode 100, Reward: 132.0\n",
      "Episode 101, Reward: 133.0\n",
      "Episode 102, Reward: 114.0\n",
      "Episode 103, Reward: 103.0\n",
      "Episode 104, Reward: 178.0\n",
      "Episode 105, Reward: 245.0\n",
      "Episode 106, Reward: 205.0\n",
      "Episode 107, Reward: 27.0\n",
      "Episode 108, Reward: 140.0\n",
      "Episode 109, Reward: 158.0\n",
      "Episode 110, Reward: 158.0\n",
      "Episode 111, Reward: 154.0\n",
      "Episode 112, Reward: 150.0\n",
      "Episode 113, Reward: 125.0\n",
      "Episode 114, Reward: 160.0\n",
      "Episode 115, Reward: 55.0\n",
      "Episode 116, Reward: 24.0\n",
      "Episode 117, Reward: 24.0\n",
      "Episode 118, Reward: 33.0\n",
      "Episode 119, Reward: 17.0\n",
      "Episode 120, Reward: 140.0\n",
      "Episode 121, Reward: 115.0\n",
      "Episode 122, Reward: 48.0\n",
      "Episode 123, Reward: 116.0\n",
      "Episode 124, Reward: 70.0\n",
      "Episode 125, Reward: 28.0\n",
      "Episode 126, Reward: 10.0\n",
      "Episode 127, Reward: 110.0\n",
      "Episode 128, Reward: 28.0\n",
      "Episode 129, Reward: 21.0\n",
      "Episode 130, Reward: 126.0\n",
      "Episode 131, Reward: 159.0\n",
      "Episode 132, Reward: 118.0\n",
      "Episode 133, Reward: 119.0\n",
      "Episode 134, Reward: 108.0\n",
      "Episode 135, Reward: 114.0\n",
      "Episode 136, Reward: 110.0\n",
      "Episode 137, Reward: 114.0\n",
      "Episode 138, Reward: 21.0\n",
      "Episode 139, Reward: 16.0\n",
      "Episode 140, Reward: 143.0\n",
      "Episode 141, Reward: 249.0\n",
      "Episode 142, Reward: 101.0\n",
      "Episode 143, Reward: 182.0\n",
      "Episode 144, Reward: 49.0\n",
      "Episode 145, Reward: 142.0\n",
      "Episode 146, Reward: 213.0\n",
      "Episode 147, Reward: 148.0\n",
      "Episode 148, Reward: 145.0\n",
      "Episode 149, Reward: 202.0\n",
      "Episode 150, Reward: 153.0\n",
      "Episode 151, Reward: 141.0\n",
      "Episode 152, Reward: 104.0\n",
      "Episode 153, Reward: 126.0\n",
      "Episode 154, Reward: 173.0\n",
      "Episode 155, Reward: 120.0\n",
      "Episode 156, Reward: 21.0\n",
      "Episode 157, Reward: 115.0\n",
      "Episode 158, Reward: 127.0\n",
      "Episode 159, Reward: 141.0\n",
      "Episode 160, Reward: 142.0\n",
      "Episode 161, Reward: 140.0\n",
      "Episode 162, Reward: 238.0\n",
      "Episode 163, Reward: 126.0\n",
      "Episode 164, Reward: 141.0\n",
      "Episode 165, Reward: 131.0\n",
      "Episode 166, Reward: 218.0\n",
      "Episode 167, Reward: 280.0\n",
      "Episode 168, Reward: 147.0\n",
      "Episode 169, Reward: 221.0\n",
      "Episode 170, Reward: 191.0\n",
      "Episode 171, Reward: 262.0\n",
      "Episode 172, Reward: 149.0\n",
      "Episode 173, Reward: 318.0\n",
      "Episode 174, Reward: 274.0\n",
      "Episode 175, Reward: 188.0\n",
      "Episode 176, Reward: 174.0\n",
      "Episode 177, Reward: 207.0\n",
      "Episode 178, Reward: 213.0\n",
      "Episode 179, Reward: 220.0\n",
      "Episode 180, Reward: 500.0\n",
      "Episode 181, Reward: 198.0\n",
      "Episode 182, Reward: 148.0\n",
      "Episode 183, Reward: 13.0\n",
      "Episode 184, Reward: 400.0\n",
      "Episode 185, Reward: 143.0\n",
      "Episode 186, Reward: 355.0\n",
      "Episode 187, Reward: 172.0\n",
      "Episode 188, Reward: 214.0\n",
      "Episode 189, Reward: 197.0\n",
      "Episode 190, Reward: 204.0\n",
      "Episode 191, Reward: 242.0\n",
      "Episode 192, Reward: 450.0\n",
      "Episode 193, Reward: 246.0\n",
      "Episode 194, Reward: 27.0\n",
      "Episode 195, Reward: 500.0\n",
      "Episode 196, Reward: 253.0\n",
      "Episode 197, Reward: 310.0\n",
      "Episode 198, Reward: 215.0\n",
      "Episode 199, Reward: 217.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Screen dimensions\n",
    "width, height = 800, 600\n",
    "x_cart_scale, pendulum_len = 150, 200\n",
    "cart_width, cart_height = 100, 5\n",
    "y_cart = height - 100\n",
    "\n",
    "# Set up the display\n",
    "scrn = pygame.display.set_mode((width, height))\n",
    "\n",
    "# Create the CartPole-v1 environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "target_update = 5\n",
    "\n",
    "# Replay buffer\n",
    "memory = deque(maxlen=10000)\n",
    "\n",
    "# Neural network for DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, env.action_space.n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize DQN and target networks\n",
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "# Function to select action based on epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    with torch.no_grad():\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = policy_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "# Function to optimize the model\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "    current_q_values = policy_net(states).gather(1, actions)\n",
    "    next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "    expected_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    loss = nn.functional.mse_loss(current_q_values.squeeze(), expected_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Reset the environment to start\n",
    "observation,_ = env.reset()\n",
    "\n",
    "# Game loop\n",
    "running = True\n",
    "for episode in range(200):  # Train over 500 episodes\n",
    "    observation,_ = env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(999):\n",
    "        scrn.fill((0, 0, 0))\n",
    "\n",
    "        # Event handling\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        # Select action using epsilon-greedy strategy\n",
    "        action = select_action(observation, epsilon)\n",
    "\n",
    "        # Step the environment forward using the selected action\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        memory.append((observation, action, reward, next_observation, done))\n",
    "\n",
    "        observation = next_observation\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Unpack observation values for visualization\n",
    "        x_cart = int(observation[0] * x_cart_scale) + width // 2  # Scale and centre the cart\n",
    "        ang = observation[2]\n",
    "\n",
    "        # Calculate pendulum position\n",
    "        x_p = x_cart + pendulum_len * math.sin(ang)\n",
    "        y_p = y_cart - pendulum_len * math.cos(ang)\n",
    "\n",
    "        # Draw the pendulum and cart\n",
    "        pygame.draw.line(scrn, (0, 0, 255), (x_p, y_p), (x_cart, y_cart), 2)\n",
    "        pygame.draw.circle(scrn, (0, 255, 0), (int(x_p), int(y_p)), 15)\n",
    "        pygame.draw.rect(scrn, (255, 0, 0), pygame.Rect(x_cart - cart_width // 2, y_cart, cart_width, cart_height))\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.update()\n",
    "\n",
    "        # Optimize the model\n",
    "        optimize_model()\n",
    "\n",
    "        # Check if the episode is terminated\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # Exit the loop if the game is closed\n",
    "        if not running:\n",
    "            break\n",
    "\n",
    "        # Pause for a brief moment (adjustable)\n",
    "        pygame.time.wait(50)  # in milliseconds\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# Close the environment and quit Pygame\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04578a4b-4517-408e-a88a-3d390f5473e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03440778,  0.38110155,  0.01173588, -0.5279972 ], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4fee0c-7d43-469b-897e-2c1adf3e2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
